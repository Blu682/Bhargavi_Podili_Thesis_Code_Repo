# -*- coding: utf-8 -*-
"""losses.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mtj99gluNy_mO5tiN-sshBYTd4NaGXAa
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile losses.py
# import tensorflow as tf
# import tensorflow_probability as tfp
# from data_utils import unpad_psd_origin_centered, unpad_kx_kz_origin_centered
# 
# def wasserstein_loss(y_true, y_pred):
#     return tf.reduce_mean(y_true * y_pred)
# 
# def gradient_penalty(discriminator, real, fake, cond):
#     alpha = tf.random.uniform([tf.shape(real)[0], 1, 1], 0.0, 1.0)
#     interpolated = real + alpha * (fake - real)
#     with tf.GradientTape() as tape:
#         tape.watch(interpolated)
#         pred = discriminator([interpolated, cond], training=True)
#     grads = tape.gradient(pred, interpolated)
#     grads_norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]) + 1e-8)
#     return tf.reduce_mean(tf.square(grads_norm - 1.0))
# 
# def feature_matching_loss(discriminator, real_psd, fake_psd, cond):
#     intermediate_layer_idx = 4  # Adjust if needed
#     feature_extractor = tf.keras.Model(
#         inputs=discriminator.input,
#         outputs=discriminator.layers[intermediate_layer_idx].output
#     )
#     real_features = feature_extractor([real_psd, cond], training=False)
#     fake_features = feature_extractor([fake_psd, cond], training=False)
#     return tf.reduce_mean(tf.abs(real_features - fake_features))
# 
# def match_tensor_shapes(t1, t2):
#     t1_shape = tf.shape(t1)
#     t2_shape = tf.shape(t2)
#     t1_transpose_shape = tf.reverse(t1_shape, axis=[0])
#     return tf.cond(
#         tf.reduce_all(tf.equal(t1_shape, t2_shape)),
#         lambda: t1,
#         lambda: tf.cond(
#             tf.reduce_all(tf.equal(t1_transpose_shape, t2_shape)),
#             lambda: tf.transpose(t1),
#             lambda: t1
#         )
#     )
# 
# def center_shift_loss(psd, kx_list, kz_list):
#     batch_size = tf.shape(psd)[0]
#     losses = []
# 
#     for i in tf.range(batch_size):
#         def compute_loss():
#             psd_linear = tf.maximum(psd[i], 1e-6, 1.0)  # shape (64, 64)
#             kx = tf.convert_to_tensor(kx_list[i], dtype=tf.float32)  # (64,)
#             kz = tf.convert_to_tensor(kz_list[i], dtype=tf.float32)  # (64,)
#             KZ, KX = tf.meshgrid(kz, kx, indexing='ij')  # Ensure correct axis layout
#             tf.debugging.assert_shapes([
#                 (psd_linear, ('H', 'W')),
#                 (KX, ('H', 'W')),
#                 (KZ, ('H', 'W'))
#             ])
#             total_mass = tf.reduce_sum(psd_linear) + 1e-6
#             cx = tf.reduce_sum(KX * psd_linear) / total_mass
#             cz = tf.reduce_sum(KZ * psd_linear) / total_mass
#             shift = tf.sqrt(cx**2 + cz**2) * 1e5
#             #tf.print("Sample", i, "→ cx:", cx, "cz:", cz, "shift:", shift)
#             return tf.where(tf.math.is_finite(shift), tf.clip_by_value(shift, 0.0, 100.0), 0.0)
#         losses.append(compute_loss())
#     return tf.reduce_mean(tf.stack(losses))
# 
# 
# def origin_peak_loss(psd, cond, kx_list, kz_list):
#     losses = []
#     for i in tf.range(tf.shape(psd)[0]):
#         h = tf.cast(cond[i, 3], tf.int32)
#         w = tf.cast(cond[i, 2], tf.int32)
#         def compute_loss():
#             unpadded_psd = tf.clip_by_value(unpad_psd_origin_centered(psd[i], h, w), 1e-6, 1.0)
#             kx, kz = unpad_kx_kz_origin_centered(kx_list[i], kz_list[i], w, h)
#             kx = tf.convert_to_tensor(kx, dtype=tf.float32)
#             kz = tf.convert_to_tensor(kz, dtype=tf.float32)
#             x0_idx = tf.argmin(tf.abs(kx))
#             z0_idx = tf.argmin(tf.abs(kz))
#             val_at_origin = unpadded_psd[z0_idx, x0_idx]
#             max_val = tf.reduce_max(unpadded_psd)
#             penalty = tf.abs(max_val - val_at_origin) / (max_val + 1e-6)
#             return tf.where(tf.math.is_finite(penalty), penalty, 1.0)
#         losses.append(tf.cond(tf.logical_and(h > 0, w > 0), compute_loss, lambda: 1.0))
#     return tf.reduce_mean(tf.stack(losses))
# 
# 
# def physics_power_loss_decay_distribution(psd, cond, kx_list, kz_list):
#     batch_size = tf.shape(psd)[0]
#     losses = []
#     for i in tf.range(batch_size):
#         h = tf.cast(cond[i, 3], tf.int32)
#         w = tf.cast(cond[i, 2], tf.int32)
#         def compute_loss():
#             unpadded_psd = tf.clip_by_value(unpad_psd_origin_centered(psd[i], h, w), 1e-6, 1.0)
#             kx, kz = unpad_kx_kz_origin_centered(kx_list[i], kz_list[i], w, h)
#             kx = tf.convert_to_tensor(kx, dtype=tf.float32)
#             kz = tf.convert_to_tensor(kz, dtype=tf.float32)
#             KX, KZ = tf.meshgrid(kx, kz, indexing='ij')
#             kcx, kcy, gamma = cond[i, 7], cond[i, 8], cond[i, 9]
#             D, L, Wval = cond[i, 4], cond[i, 0], cond[i, 1]
#             k_squared = tf.square(KX / (kcx + 1e-12)) + tf.square(KZ / (kcy + 1e-12))
#             k_term = tf.pow(1.0 + k_squared, gamma + 1e-12)
#             reference_psd = D * L * Wval * 1e6 / tf.sqrt(k_term + 1e-12)
#             #unpadded_psd = tf.maximum(unpadded_psd, 1e-12)
#             reference_psd = tf.clip_by_value(reference_psd, 1e-6, 1.0)
#             reference_psd = match_tensor_shapes(reference_psd, unpadded_psd)
#             reference_psd /= tf.reduce_max(reference_psd) + 1e-6
#             unpadded_psd /= tf.reduce_max(unpadded_psd) + 1e-6
# 
#             ref_log = tf.math.log(reference_psd + 1e-6) / tf.math.log(10.0)
#             psd_log = unpadded_psd
#             ref_log = match_tensor_shapes(ref_log, psd_log)
#             r = tf.sqrt((KX / (kcx + 1e-12))**2 + (KZ / (kcy + 1e-12))**2)
#             r = match_tensor_shapes(r, psd_log)
#             mask = tf.greater(r, 1.0)
#             r_flat = tf.boolean_mask(r, mask)
#             ref_flat = tf.boolean_mask(ref_log, mask)
#             psd_flat = tf.boolean_mask(psd_log, mask)
#             def safe_loss():
#                 nbins = 40
#                 r_bins = tf.linspace(tf.reduce_min(r_flat), tf.reduce_max(r_flat), nbins + 1)
#                 indices = tf.raw_ops.Bucketize(
#                     input=r_flat,
#                     boundaries=r_bins.numpy().tolist()
#                 )
#                 indices = tf.cast(indices, tf.int32)
#                 def radial_profile(log_vals):
#                     sums = tf.math.unsorted_segment_sum(log_vals, indices, nbins + 2)
#                     counts = tf.math.unsorted_segment_sum(tf.ones_like(log_vals), indices, nbins + 2)
#                     avg = sums / tf.maximum(counts, 1.0)
#                     return avg[1:nbins + 1]
#                 ref_profile = radial_profile(ref_flat)
#                 gen_profile = radial_profile(psd_flat)
#                 epsilon = 1e-8
#                 ref_dist = tf.clip_by_value(ref_profile / tf.reduce_sum(ref_profile + epsilon), epsilon, 1.0)
#                 gen_dist = tf.clip_by_value(gen_profile / tf.reduce_sum(gen_profile + epsilon), epsilon, 1.0)
#                 kl = tf.reduce_sum(ref_dist * tf.math.log((ref_dist + epsilon) / (gen_dist + epsilon)))
#                 mse = tf.reduce_mean(tf.square(ref_profile - gen_profile))
#                 hybrid_loss = 0.7 * kl + 0.3 * mse
#                 raw_psd = tf.boolean_mask(unpadded_psd, mask)
#                 power_penalty = tf.abs(1.0 - tf.reduce_mean(raw_psd))
#                 total_loss = hybrid_loss + 0.1 * power_penalty
#                 return tf.where(tf.math.is_finite(total_loss), total_loss, 0.0)
#             return tf.cond(tf.size(r_flat) > 0, safe_loss, lambda: 0.0)
#         loss = tf.cond(tf.logical_and(h > 0, w > 0), compute_loss, lambda: 0.0)
#         losses.append(loss)
#     return tf.reduce_mean(tf.stack(losses))
# 
# def spectral_smoothness_loss(psd_batch):
#     losses = []
#     for i in tf.range(tf.shape(psd_batch)[0]):
#         #psd = psd_batch[i]
#         psd = tf.clip_by_value(psd_batch[i], 1e-6, 1.0)
#         dx = tf.abs(psd[:, 1:] - psd[:, :-1])
#         dy = tf.abs(psd[1:, :] - psd[:-1, :])
#         smoothness = tf.reduce_mean(dx) + tf.reduce_mean(dy)
#         losses.append(smoothness)
#     return tf.reduce_mean(tf.stack(losses))
# 
# def diversity_loss(generated_batch):
#     batch_size = tf.shape(generated_batch)[0]
#     loss = 0.0
#     count = 0
#     for i in range(batch_size):
#         for j in range(i + 1, batch_size):
#             diff = generated_batch[i] - generated_batch[j]
#             loss += tf.reduce_mean(tf.square(diff))
#             count += 1
#     return loss / tf.cast(count, tf.float32)
# 
# 
# def q1_q2_asymmetry_penalty(psd, cond, epsilon=1e-6):
#     """
#     Penalize asymmetry between Q1 and mirrored Q2 in the generated PSD.
#     Assumes origin is at index [31, 31] (i.e., center of 64x64).
#     Mirrors Q2 along the kx-axis (horizontal flip).
#     Only compares the central h//2 × w//2 unpadded region of Q1 and mirrored Q2.
#     """
#     batch_size = tf.shape(psd)[0]
#     losses = []
#     epsilon = 1e-6
#     for i in tf.range(batch_size):
#         h = tf.cast(cond[i, 3], tf.int32)  # height (kz dim)
#         w = tf.cast(cond[i, 2], tf.int32)  # width  (kx dim)
#         h_half = (h+1) // 2
#         w_half = (w+1) // 2
#         def compute_loss():
#             # From padded 64x64, extract Q1 and Q2 (top-right and top-left quadrants)
#             # Q1 = top-right quadrant (excluding origin column)
#             #q1 = psd[i, :31, 33:64]  # shape (B, 31, 31)
#             q1 = tf.clip_by_value(psd[i, :31, 33:64], 1e-6, 1.0)
#             # Q2 = top-left quadrant (excluding origin column)
#             #q2 = psd[i, :31, :31]  # shape (B, 31, 31)
#             q2 = tf.clip_by_value(psd[i, :31, :31], 1e-6, 1.0)
#             # Mirror Q2 along the kx-axis (horizontal flip)
#             q2_mirrored = tf.reverse(q2, axis=[1])
#             # Crop to valid unpadded subregion
#             q1_crop = q1[-h_half:, :w_half]
#             q2_crop = q2_mirrored[-h_half:, :w_half]
#             diff = tf.abs(q1_crop - q2_crop)
#             #norm = (tf.abs(q1_crop) + tf.abs(q2_crop)) / 2.0 + epsilon
#             norm = tf.maximum((tf.abs(q1_crop) + tf.abs(q2_crop)) / 2.0, 1e-6)
#             rel_diff = diff / norm
#             return tf.reduce_mean(rel_diff)
#         loss = tf.cond(tf.logical_and(h > 0, w > 0), compute_loss, lambda: 0.0)
#         losses.append(loss)
#     return tf.reduce_mean(tf.stack(losses))
# 
#