# -*- coding: utf-8 -*-
"""generator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PdiozmsgZWPqezVA4LzuQ4TrG7xYbwDE
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile generator.py
# import tensorflow as tf
# from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, LeakyReLU, concatenate, Conv2D, Conv2DTranspose, Layer
# from tensorflow.keras.models import Model
# from tensorflow.keras.saving import register_keras_serializable
# 
# LATENT_DIM = 100
# max_H, max_W = 64, 64  # For discriminator input
# 
# @register_keras_serializable()
# class SqueezeLayer(tf.keras.layers.Layer):
#     def call(self, inputs):
#         return tf.squeeze(inputs, axis=-1)
# 
# @register_keras_serializable()
# class OriginSymmetryLayer(Layer):
#     def call(self, inputs):
#         top_half, kx, kz, cond = inputs
#         # Split Q2 (left half) and Q1 (right half)
#         q2 = top_half[:, :32, :32, :]  # Top-left
#         q1 = top_half[:, :32, 32:, :]  # Top-right
#         # ---- Q3 Construction ----
#         q1_crop = q1[:, :31, :31, :]                         # Exclude last row and column
#         q3_core = tf.reverse(q1_crop, axis=[1, 2])          # Mirror top-left of Q1 (31x31)
#         q2_right_col = tf.reverse(q2[:, :31, 31:32, :], axis=[1])  # Vertical flip of last col of Q2 (excluding origin)
#         q3_top = tf.concat([q3_core, q2_right_col], axis=2)        # (31,32)
#         q3_last_row = tf.zeros_like(q3_top[:, :1, :, :])           # Last row (to be learned)
#         q3_full = tf.concat([q3_top, q3_last_row], axis=1)         # (32,32)
#         # ---- Q4 Construction ----
#         q2_crop = q2[:, :31, :31, :]                      # Exclude last row and column
#         q4_core = tf.reverse(q2_crop, axis=[1, 2])       # Mirror top-left of Q2 (31x31)
#         q4_last_col = tf.zeros_like(q4_core[:, :, :1, :])  # Last column (to be learned)
#         q4_partial = tf.concat([q4_core, q4_last_col], axis=2)  # (31,32)
#         q4_last_row = tf.zeros_like(q4_partial[:, :1, :, :])    # Last row (to be learned)
#         q4_full = tf.concat([q4_partial, q4_last_row], axis=1)  # (32,32)
#         # Ensure q1 has enough rows
#         q1_rows = tf.shape(q1)[1]
#         q1_cols = tf.shape(q1)[2]
#         # Get mirrored Q2 last row
#         q2_last_row = tf.reverse(q2[:, 31:32, :31, :], axis=[2])  # (B, 1, 31, 1)
#         # Get Q1's last col of the last row, if it exists
#         q1_last_col = q1[:, 31:32, 31:32, :]  # (B, 1, 1, 1) if available
#         # Concatenate horizontally
#         q1_last_row = tf.concat([q2_last_row, q1_last_col], axis=2)  # (B, 1, 32, 1)
#         # Concatenate vertically
#         q1_fixed = tf.concat([q1[:, :31, :, :], q1_last_row], axis=1)  # (B, 32, 32, 1)
#         # ---- Combine Quadrants ----
#         top = tf.concat([q2, q1_fixed], axis=2)                       # (B, 32, 64, 1)
#         bottom = tf.concat([q3_full, q4_full], axis=2)         # (B, 32, 64, 1)
#         full = tf.concat([top, bottom], axis=1)                # (B, 64, 64, 1)
#         return full
# 
# 
# def build_dynamic_generator():
#     noise = Input(shape=(LATENT_DIM,), name='noise_input')
#     cond_input = Input(shape=(10,), name='condition_input')
#     kx_input = Input(shape=(64,), name='kx_input')
#     kz_input = Input(shape=(64,), name='kz_input')
# 
#     cond_encoded = Dense(64, activation='relu')(cond_input)
#     merged = concatenate([noise, cond_encoded])
# 
#     x = Dense(4 * 4 * 128)(merged)
#     x = LeakyReLU(0.2)(x)
#     x = Reshape((4, 4, 128))(x)
# 
#     x = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)
#     x = LeakyReLU(0.2)(x)
#     x = BatchNormalization()(x)
#     x = Dropout(0.3)(x)
# 
#     x = Conv2DTranspose(32, kernel_size=4, strides=2, padding='same')(x)
#     x = LeakyReLU(0.2)(x)
#     x = BatchNormalization()(x)
# 
#     x = Conv2DTranspose(16, kernel_size=4, strides=2, padding='same')(x)  # -> 32x32
#     x = LeakyReLU(0.2)(x)
#     x = BatchNormalization()(x)
# 
#     x = Conv2DTranspose(8, kernel_size=4, strides=2, padding='same')(x)  # â†’ 64x64
#     x = LeakyReLU(0.2)(x)
#     x = BatchNormalization()(x)
# 
#     q1q2 = Conv2DTranspose(1, kernel_size=3, strides=1, padding='same', activation='tanh')(x)
#     full = OriginSymmetryLayer()([q1q2, kx_input, kz_input, cond_input])
#     full = tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=-1), output_shape=(64, 64))(full)
#     return Model(inputs=[noise, cond_input, kx_input, kz_input], outputs=full)
#