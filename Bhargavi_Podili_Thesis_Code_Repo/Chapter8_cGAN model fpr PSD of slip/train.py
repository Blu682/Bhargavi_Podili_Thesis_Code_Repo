# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mtj99gluNy_mO5tiN-sshBYTd4NaGXAa
"""

# Commented out IPython magic to ensure Python compatibility.
# # 4 #
# %%writefile train.py
# import numpy as np
# import tensorflow as tf
# from tensorflow.keras.optimizers import Adam
# from losses import (wasserstein_loss, gradient_penalty, feature_matching_loss,
#                     center_shift_loss, origin_peak_loss, physics_power_loss_decay_distribution,
#                     spectral_smoothness_loss, diversity_loss, q1_q2_asymmetry_penalty)
# from tensorflow.summary import create_file_writer
# from evaluate import compare_real_vs_generated_single
# 
# LATENT_DIM = 100
# BATCH_SIZE = 32
# EPOCHS = 200
# LEARNING_RATE_G = 1e-4  # Generator LR
# LEARNING_RATE_D = 2e-5  # Discriminator LR (TTUR)
# LEARNING_RATE = 1e-4
# log_dir = "/content/drive/MyDrive/Slip/logs"
# 
# class EMA:
#     def __init__(self, model, decay=0.9995):
#         self.model = model
#         self.ema_model = tf.keras.models.clone_model(model)
#         self.decay = decay
#         self._initialize()
# 
#     def _initialize(self):
#         for w, ema_w in zip(self.model.weights, self.ema_model.weights):
#             ema_w.assign(w)
# 
#     def update(self):
#         for w, ema_w in zip(self.model.weights, self.ema_model.weights):
#             ema_w.assign(self.decay * ema_w + (1.0 - self.decay) * w)
# 
#     def apply_ema_weights(self):
#         self.backup = [w.numpy() for w in self.model.weights]
#         for w, ema_w in zip(self.model.weights, self.ema_model.weights):
#             w.assign(ema_w)
# 
#     def reset_old_weights(self):
#         for w, b in zip(self.model.weights, self.backup):
#             w.assign(b)
# 
# def train_cgan(generator, discriminator, psd_data, conditions, kx_list, kz_list):
#     dataset = PSDDataset(psd_data, conditions, kx_list, kz_list, BATCH_SIZE)
#     real_label = -tf.random.uniform((BATCH_SIZE, 1), 0.8, 1.0)
#     fake_label = tf.random.uniform((BATCH_SIZE, 1), 0.8, 1.0)
#     #real_label = -0.9 * np.ones((BATCH_SIZE, 1))
#     #fake_label = 0.9 * np.ones((BATCH_SIZE, 1))
#     best_loss = np.inf
#     patience = 50
#     patience_counter = 0
#     writer = create_file_writer(log_dir)
#     d_optimizer = Adam(LEARNING_RATE_D, beta_1=0.5, clipvalue=0.5)
#     g_optimizer = Adam(LEARNING_RATE_G, beta_1=0.5, clipnorm=1.0)
#     ema = EMA(generator)
#     n_critic=2
#     for epoch in range(EPOCHS):
#         d_losses, g_losses = [], []
#         print(f"\nEpoch {epoch + 1}/{EPOCHS}")
#         if (epoch + 1) % 10 == 0:
#             sample_index = np.random.randint(0, len(psd_data))
#             print(f"Visualizing sample index: {sample_index}")
#             compare_real_vs_generated_single(
#                 sample_index, generator, psd_data, conditions, kx_list, kz_list
#             )
#         for i in range(len(dataset)):
#             [z, cond], real_psd, kx_batch, kz_batch = dataset[i]
#             z = tf.convert_to_tensor(z, dtype=tf.float32)
#             real_psd = tf.convert_to_tensor(real_psd, dtype=tf.float32)
#             cond = tf.convert_to_tensor(cond, dtype=tf.float32)
#             kx_batch = tf.convert_to_tensor(np.stack(kx_batch), dtype=tf.float32)
#             kz_batch = tf.convert_to_tensor(np.stack(kz_batch), dtype=tf.float32)
#             for _ in range(n_critic):
#                 with tf.GradientTape(persistent=True) as tape:
#                     gen_psd = generator([z, cond, kx_batch, kz_batch], training=True)
#                     gen_psd = tf.clip_by_value(gen_psd, 0.0, 1000.0)
#                     d_real = discriminator([real_psd, cond])
#                     d_fake = discriminator([gen_psd, cond])
#                     d_real_loss = wasserstein_loss(real_label[:len(d_real)], d_real)
#                     d_fake_loss = wasserstein_loss(fake_label[:len(d_fake)], d_fake)
#                     gp = gradient_penalty(discriminator, real_psd, gen_psd, cond)
#                     d_phys = physics_power_loss_decay_distribution(real_psd, cond, kx_batch, kz_batch)
#                     #gp = tf.clip_by_value(gp, 0.0, 10.0)
#                     gp = tf.where(tf.math.is_finite(gp), tf.clip_by_value(gp, 0.0, 10.0), 0.0)
#                     #lambda_phys = tf.minimum(0.5, 0.2 + 0.05 * epoch)
#                     d_loss = d_real_loss + d_fake_loss + 0.1 * gp + 0.5 * d_phys
#                 if not tf.math.reduce_any(tf.math.is_nan(d_loss)):
#                     grads = tape.gradient(d_loss, discriminator.trainable_variables)
#                     grads, _ = tf.clip_by_global_norm(grads, 5.0)
#                     d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))
#                     d_losses.append(d_loss.numpy())
#                 del tape
#             z = tf.random.normal((len(cond), LATENT_DIM))
#             with tf.GradientTape() as tape:
#                 gen_psd = generator([z, cond, kx_batch, kz_batch], training=True)
#                 gen_psd = tf.clip_by_value(gen_psd, 0.0, 1000.0)
#                 #q1q2_tensor = gen_outputs["q1q2"]
#                 d_gen = discriminator([gen_psd, cond])
#                 g_adv = wasserstein_loss(real_label[:len(d_gen)], d_gen)
#                 g_phys = physics_power_loss_decay_distribution(gen_psd, cond, kx_batch, kz_batch)
#                 g_center = center_shift_loss(gen_psd, kx_batch, kz_batch)
#                 g_origin_peak = origin_peak_loss(gen_psd, cond, kx_batch, kz_batch)
#                 g_fm = feature_matching_loss(discriminator, real_psd, gen_psd, cond)
#                 g_smooth = spectral_smoothness_loss(gen_psd)
#                 g_diverse = diversity_loss(gen_psd)
#                 g_qbal = q1_q2_asymmetry_penalty(gen_psd, cond)
#                 g_adv = tf.where(tf.math.is_finite(g_adv), g_adv, 0.0)
#                 #tf.print("g_adv:", g_adv, "g_phys:", g_phys, "g_center:", g_center,
#                 #          "g_origin_peak:", g_origin_peak, "g_fm:", g_fm,
#                 #          "g_smooth:", g_smooth, "g_diverse:", g_diverse, "g_qbal:", g_qbal)
# 
#                 g_phys = tf.where(tf.math.is_finite(g_phys), g_phys, 0.0)
#                 g_center = tf.where(tf.math.is_finite(g_center), g_center, 0.0)
#                 g_origin_peak = tf.where(tf.math.is_finite(g_origin_peak), g_origin_peak, 0.0)
#                 g_fm = tf.where(tf.math.is_finite(g_fm), g_fm, 0.0)
#                 g_smooth = tf.where(tf.math.is_finite(g_smooth), g_smooth, 0.0)
#                 g_diverse = tf.where(tf.math.is_finite(g_diverse), g_diverse, 0.0)
#                 g_qbal = tf.where(tf.math.is_finite(g_qbal), g_qbal, 0.0)
#                 lambda_phys = tf.minimum(0.5, 0.1 + 0.01 * epoch)
#                 #lambda_phys = tf.minimum(2.0, 1.0 + 0.02 * epoch)
#                 g_loss = (0.08 * g_adv +
#                           lambda_phys * g_phys +
#                           1.0 * g_center +
#                           0.8 * g_origin_peak +
#                           0.3 * g_fm +
#                           0.3 * g_smooth +
#                           0.2 * g_diverse +
#                           1.5 * g_qbal)
#             if not tf.math.reduce_any(tf.math.is_nan(g_loss)):
#                 grads = tape.gradient(g_loss, generator.trainable_variables)
#                 grads, _ = tf.clip_by_global_norm(grads, 5.0)
#                 g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))
#                 ema.update()
#                 g_losses.append(g_loss.numpy())
#         print(f"D Loss: {np.mean(d_losses):.4f}, G Loss: {np.mean(g_losses):.4f}")
#         print(f"d_real_loss: {d_real_loss:.3f}, d_fake_loss: {d_fake_loss:.3f}, d_gp: {gp:.3f}, d_phys: {d_phys:.3f}")
#         print(f"g_adv: {g_adv:.3f}, g_phys: {g_phys:.3f}, g_peak: {g_origin_peak:.3f}, g_center: {g_center:.3f}, g_fm: {g_fm:.3f}, g_smooth: {g_smooth:.3f}, g_diverse: {g_diverse:.3f}, g_qbal: {g_qbal:.3f}")
#         with writer.as_default():
#             tf.summary.scalar("d_loss", np.mean(d_losses), step=epoch)
#             tf.summary.scalar("g_loss", np.mean(g_losses), step=epoch)
#             tf.summary.scalar("g_adv", g_adv, step=epoch)
#             tf.summary.scalar("g_phys", g_phys, step=epoch)
#             tf.summary.scalar("g_center", g_center, step=epoch)
#             tf.summary.scalar("g_smooth", g_smooth, step=epoch)
#             tf.summary.scalar("g_diverse", g_diverse, step=epoch)
#             tf.summary.scalar("g_qbal", g_qbal, step=epoch)
#             writer.flush()
#         ema.apply_ema_weights()
#         generator.save('/content/drive/MyDrive/Slip/generator_model_epoch_final.keras')
#         if np.mean(g_losses) < best_loss:
#             best_loss = np.mean(g_losses)
#             generator.save("/content/drive/MyDrive/Slip/best_generator.keras")
#         #    patience_counter = 0
#         #else:
#             #patience_counter += 1
#             #if patience_counter >= patience:
#             #    print("Early stopping triggered.")
#             #    break
#         ema.reset_old_weights()
# 
# 
# class PSDDataset(tf.keras.utils.Sequence):
#     def __init__(self, psd_data, conditions, kx_list, kz_list, batch_size):
#         self.max_vals = np.max(psd_data, axis=(1, 2), keepdims=True)
#         self.psd_data = psd_data / (self.max_vals)
#         self.conditions = conditions
#         self.kx_list = kx_list
#         self.kz_list = kz_list
#         self.batch_size = batch_size
# 
#     def __len__(self):
#         return int(np.ceil(len(self.psd_data) / self.batch_size))
# 
#     def __getitem__(self, idx):
#         batch_psd = self.psd_data[idx * self.batch_size:(idx + 1) * self.batch_size]
#         batch_cond = self.conditions[idx * self.batch_size:(idx + 1) * self.batch_size]
#         batch_kx = self.kx_list[idx * self.batch_size:(idx + 1) * self.batch_size]
#         batch_kz = self.kz_list[idx * self.batch_size:(idx + 1) * self.batch_size]
#         z = np.random.normal(0, 1, (len(batch_psd), LATENT_DIM))
#         return [z, batch_cond], batch_psd, batch_kx, batch_kz
#